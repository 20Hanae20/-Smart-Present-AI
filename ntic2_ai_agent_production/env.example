# Configuration LLM Provider
# Options: ollama, groq, huggingface, openai
LLM_PROVIDER=ollama

# Configuration Ollama (local, 100% gratuit)
# URL de base d'Ollama (par défaut: http://ollama:11434)
OLLAMA_BASE_URL=http://ollama:11434

# Configuration Groq API (cloud, gratuit, très rapide)
# Obtenez une clé API gratuite sur: https://console.groq.com/
GROQ_API_KEY=your_groq_api_key_here

# Configuration Hugging Face Inference API (cloud, gratuit)
# Obtenez un token sur: https://huggingface.co/settings/tokens
# Optionnel mais recommandé pour éviter les rate limits
HF_API_KEY=

# Configuration OpenAI (optionnel, seulement si utilisé comme fallback)
# Note: OpenAI est payant, utilisez plutôt Ollama ou Groq pour une solution gratuite
OPENAI_API_KEY=your_openai_api_key_here

# Configuration PostgreSQL (déjà configurée dans docker-compose.yml)
# POSTGRES_DB=ntic2
# POSTGRES_USER=ntic
# POSTGRES_PASSWORD=ntic
