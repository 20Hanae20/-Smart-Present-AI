# ğŸ¤– Assistant ISTA NTIC - Documentation ComplÃ¨te

## ğŸ“‹ Table des MatiÃ¨res

1. [Vue d'ensemble](#vue-densemble)
2. [Architecture](#architecture)
3. [Technologies UtilisÃ©es](#technologies-utilisÃ©es)
4. [Structure du Projet](#structure-du-projet)
5. [Installation](#installation)
6. [Configuration](#configuration)
7. [Utilisation](#utilisation)
8. [API Documentation](#api-documentation)
9. [RAG Pipeline](#rag-pipeline)
10. [Agent Core](#agent-core)
11. [DÃ©ploiement](#dÃ©ploiement)
12. [Troubleshooting](#troubleshooting)

---

## ğŸ¯ Vue d'ensemble

**Assistant ISTA NTIC** est un systÃ¨me d'assistant intelligent basÃ© sur l'IA pour l'Institut SpÃ©cialisÃ© de Technologie AppliquÃ©e (ISTA) NTIC Sidi Maarouf. Le systÃ¨me utilise la technologie RAG (Retrieval-Augmented Generation) pour fournir des rÃ©ponses prÃ©cises basÃ©es sur les donnÃ©es du site web officiel de l'Ã©tablissement.

### FonctionnalitÃ©s Principales

- âœ… **Chat intelligent multilingue** (FranÃ§ais, Anglais, Arabe, Espagnol)
- âœ… **RAG (Retrieval-Augmented Generation)** pour des rÃ©ponses basÃ©es sur les donnÃ©es du site
- âœ… **Support multi-LLM** : Ollama (local), Groq (cloud), Hugging Face (cloud), OpenAI (cloud)
- âœ… **MÃ©moire conversationnelle** persistante avec PostgreSQL
- âœ… **Ingestion automatique** du contenu du site web
- âœ… **Interface utilisateur moderne** avec React
- âœ… **API RESTful** complÃ¨te
- âœ… **ObservabilitÃ©** avec logging dÃ©taillÃ©

---

## ğŸ—ï¸ Architecture

### Architecture Globale

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend      â”‚  React + Vite
â”‚   (Port 8080)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ HTTP/REST
         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Backend       â”‚  Flask + Python
â”‚   (Port 5000)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚         â”‚          â”‚              â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â” â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚PostgreSQLâ”‚ â”‚ChromaDBâ”‚ â”‚  Ollama   â”‚ â”‚  LLM APIs   â”‚
â”‚ (Memory) â”‚ â”‚  (RAG) â”‚ â”‚  (Local)  â”‚ â”‚ (Groq/HF)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Flux de DonnÃ©es

1. **RequÃªte Utilisateur** â†’ Frontend (React)
2. **API Call** â†’ Backend Flask (`/api/chat`)
3. **Agent Core** â†’ Traitement de la requÃªte
4. **RAG Pipeline** â†’ Recherche dans ChromaDB
5. **LLM Provider** â†’ GÃ©nÃ©ration de la rÃ©ponse
6. **PostgreSQL** â†’ Sauvegarde de l'historique
7. **RÃ©ponse** â†’ Frontend avec sources

---

## ğŸ› ï¸ Technologies UtilisÃ©es

### Backend

#### Framework & Core
- **Python 3.11** - Langage de programmation
- **Flask 2.x** - Framework web lÃ©ger
- **Werkzeug** - Utilitaires WSGI

#### Base de DonnÃ©es
- **PostgreSQL 15** - Base de donnÃ©es relationnelle pour la mÃ©moire conversationnelle
- **psycopg2-binary** - Driver PostgreSQL pour Python
- **ChromaDB** - Base de donnÃ©es vectorielle pour RAG

#### IA & LLM
- **OpenAI** - API OpenAI (optionnel, payant)
- **Ollama** - LLM local gratuit (Llama 3.2, Mistral, etc.)
- **Groq API** - LLM cloud gratuit et rapide
- **Hugging Face Inference API** - LLM cloud gratuit
- **sentence-transformers** - ModÃ¨les d'embeddings locaux

#### RAG & Embeddings
- **ChromaDB** - Base de donnÃ©es vectorielle
- **BeautifulSoup4** - Parsing HTML
- **requests** - RequÃªtes HTTP
- **langchain** - Framework pour applications LLM

#### Traitement de Documents
- **PyPDF2** - Lecture de fichiers PDF
- **pdfplumber** - Extraction avancÃ©e de PDF
- **openpyxl** - Traitement de fichiers Excel
- **pandas** - Manipulation de donnÃ©es
- **Pillow** - Traitement d'images
- **pytesseract** - OCR (Optical Character Recognition)

#### Utilitaires
- **python-dotenv** - Gestion des variables d'environnement
- **logging** - SystÃ¨me de logs intÃ©grÃ©

### Frontend

#### Framework & Build
- **React 18.2.0** - BibliothÃ¨que UI
- **React DOM 18.2.0** - Rendu React
- **Vite 5.1.0** - Build tool et dev server
- **@vitejs/plugin-react** - Plugin React pour Vite

#### Styling
- **CSS3** - Styles personnalisÃ©s
- **Responsive Design** - Interface adaptative

### Infrastructure

#### Containerisation
- **Docker** - Containerisation
- **Docker Compose** - Orchestration multi-containers
- **Nginx** - Serveur web pour le frontend (production)

#### Services
- **PostgreSQL 15** - Base de donnÃ©es
- **Ollama** - Service LLM local
- **ChromaDB** - Service de base vectorielle

---

## ğŸ“ Structure du Projet

```
ntic2_ai_agent_production/
â”‚
â”œâ”€â”€ backend/                    # Application backend Python
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ main.py            # Point d'entrÃ©e Flask, routes API
â”‚   â”‚   â”œâ”€â”€ memory.py          # Gestion mÃ©moire conversationnelle
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ agent/             # Module agent IA
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â””â”€â”€ core.py        # CÅ“ur de l'agent (LLM, RAG, prompts)
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ db/                # Module base de donnÃ©es
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ database.py    # Connexion PostgreSQL
â”‚   â”‚   â”‚   â””â”€â”€ seed.py        # DonnÃ©es initiales
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ observability/     # Module observabilitÃ©
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â””â”€â”€ logger.py      # Logging et monitoring
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ rag/              # Module RAG
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ ingest.py     # Ingestion du contenu web
â”‚   â”‚       â””â”€â”€ pipeline.py    # Pipeline RAG (recherche, embeddings)
â”‚   â”‚
â”‚   â”œâ”€â”€ Dockerfile             # Image Docker backend
â”‚   â”œâ”€â”€ requirements.txt       # DÃ©pendances Python
â”‚   â”œâ”€â”€ check_and_ingest.py    # Script d'ingestion
â”‚   â””â”€â”€ diagnostic_systeme.py  # Script de diagnostic
â”‚
â”œâ”€â”€ frontend/                   # Application frontend React
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.jsx            # Composant principal
â”‚   â”‚   â”œâ”€â”€ main.jsx           # Point d'entrÃ©e React
â”‚   â”‚   â”œâ”€â”€ styles.css         # Styles globaux
â”‚   â”‚   â””â”€â”€ components/
â”‚   â”‚       â””â”€â”€ Chat.jsx       # Composant chat
â”‚   â”‚
â”‚   â”œâ”€â”€ Dockerfile             # Image Docker frontend (production)
â”‚   â”œâ”€â”€ Dockerfile.dev         # Image Docker frontend (dev)
â”‚   â”œâ”€â”€ nginx.conf             # Configuration Nginx
â”‚   â”œâ”€â”€ package.json           # DÃ©pendances Node.js
â”‚   â””â”€â”€ vite.config.js         # Configuration Vite
â”‚
â”œâ”€â”€ secrets/                    # Secrets (non versionnÃ©s)
â”‚   â”œâ”€â”€ openai_api_key         # ClÃ© API OpenAI
â”‚   â””â”€â”€ openai_api_key.txt     # ClÃ© API OpenAI (backup)
â”‚
â”œâ”€â”€ chroma_db/                  # Base de donnÃ©es ChromaDB (gÃ©nÃ©rÃ©e)
â”‚   â””â”€â”€ chroma.sqlite3         # Fichier SQLite de ChromaDB
â”‚
â”œâ”€â”€ docker-compose.yml          # Configuration Docker Compose
â”œâ”€â”€ .env                        # Variables d'environnement (non versionnÃ©)
â”œâ”€â”€ env.example                 # Exemple de fichier .env
â”œâ”€â”€ init.sql                    # Script d'initialisation PostgreSQL
â”œâ”€â”€ init_ollama.sh             # Script d'initialisation Ollama
â”‚
â””â”€â”€ README.md                   # Cette documentation
```

---

## ğŸš€ Installation

### PrÃ©requis

- **Docker** et **Docker Compose** installÃ©s
- **Git** pour cloner le projet
- **8GB RAM minimum** (recommandÃ© 16GB pour Ollama)
- **Ports disponibles** : 5000 (backend), 8080 (frontend), 5432 (PostgreSQL), 11434 (Ollama)

### Installation avec Docker (RecommandÃ©)

1. **Cloner le projet**
```bash
git clone <repository-url>
cd ntic2_ai_agent_production
```

2. **Configurer les variables d'environnement**
```bash
cp env.example .env
# Ã‰diter .env avec vos clÃ©s API
```

3. **Configurer les secrets**
```bash
# CrÃ©er le fichier secrets/openai_api_key avec votre clÃ© OpenAI
echo "votre_cle_openai" > secrets/openai_api_key
```

4. **DÃ©marrer les services**
```bash
docker-compose up -d
```

5. **Initialiser Ollama (si utilisÃ©)**
```bash
docker-compose exec ollama ollama pull llama3.2:latest
```

6. **IngÃ©rer les donnÃ©es du site**
```bash
docker-compose exec backend python -m app.rag.ingest
```

### Installation Locale (Sans Docker)

#### Backend

1. **Installer Python 3.11+**
```bash
python --version  # VÃ©rifier la version
```

2. **CrÃ©er un environnement virtuel**
```bash
cd backend
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate  # Windows
```

3. **Installer les dÃ©pendances**
```bash
pip install -r requirements.txt
```

4. **Configurer PostgreSQL**
   - Installer PostgreSQL 15
   - CrÃ©er la base de donnÃ©es `ntic2`
   - ExÃ©cuter `init.sql`

5. **Configurer les variables d'environnement**
```bash
# CrÃ©er .env Ã  la racine du projet
OPENAI_API_KEY=votre_cle
LLM_PROVIDER=ollama
OLLAMA_BASE_URL=http://localhost:11434
```

6. **DÃ©marrer le serveur**
```bash
cd backend
flask run --host=0.0.0.0 --port=5000
```

#### Frontend

1. **Installer Node.js 18+**
```bash
node --version  # VÃ©rifier la version
```

2. **Installer les dÃ©pendances**
```bash
cd frontend
npm install
```

3. **DÃ©marrer le serveur de dÃ©veloppement**
```bash
npm run dev
```

4. **Build de production**
```bash
npm run build
```

---

## âš™ï¸ Configuration

### Variables d'Environnement (.env)

```env
# Provider LLM (ollama, groq, huggingface, openai)
LLM_PROVIDER=ollama

# Configuration Ollama (local, gratuit)
OLLAMA_BASE_URL=http://ollama:11434
# Pour usage local: http://localhost:11434

# Configuration Groq API (cloud, gratuit, rapide)
GROQ_API_KEY=votre_cle_groq

# Configuration Hugging Face (cloud, gratuit)
HF_API_KEY=votre_token_hf

# Configuration OpenAI (optionnel, payant)
OPENAI_API_KEY=votre_cle_openai

# Configuration PostgreSQL
POSTGRES_HOST=postgres
POSTGRES_PORT=5432
POSTGRES_DB=ntic2
POSTGRES_USER=ntic
POSTGRES_PASSWORD=ntic
```

### Configuration des LLM Providers

#### 1. Ollama (RecommandÃ© - Local, Gratuit)

```bash
# Installer Ollama
# Windows: TÃ©lÃ©charger depuis https://ollama.ai
# Linux/Mac: curl -fsSL https://ollama.ai/install.sh | sh

# TÃ©lÃ©charger un modÃ¨le
ollama pull llama3.2:latest
# ou
ollama pull mistral:latest
```

**Avantages** :
- âœ… 100% gratuit
- âœ… Fonctionne hors ligne
- âœ… DonnÃ©es privÃ©es
- âœ… Pas de limite de requÃªtes

#### 2. Groq (Cloud, Gratuit, Rapide)

1. CrÃ©er un compte sur https://console.groq.com/
2. GÃ©nÃ©rer une clÃ© API
3. Ajouter dans `.env` : `GROQ_API_KEY=votre_cle`

**Avantages** :
- âœ… Gratuit avec quota gÃ©nÃ©reux
- âœ… TrÃ¨s rapide (infrastructure optimisÃ©e)
- âœ… Pas d'installation locale

#### 3. Hugging Face (Cloud, Gratuit)

1. CrÃ©er un compte sur https://huggingface.co/
2. GÃ©nÃ©rer un token sur https://huggingface.co/settings/tokens
3. Ajouter dans `.env` : `HF_API_KEY=votre_token`

**Avantages** :
- âœ… Gratuit
- âœ… AccÃ¨s Ã  de nombreux modÃ¨les
- âœ… Optionnel (fonctionne sans token mais avec rate limits)

#### 4. OpenAI (Cloud, Payant)

1. CrÃ©er un compte sur https://platform.openai.com/
2. GÃ©nÃ©rer une clÃ© API
3. Ajouter dans `.env` : `OPENAI_API_KEY=votre_cle`

**Avantages** :
- âœ… ModÃ¨les trÃ¨s performants (GPT-4, GPT-4o-mini)
- âœ… API stable et fiable

### Configuration ChromaDB

ChromaDB est automatiquement initialisÃ© lors de la premiÃ¨re ingestion. Les donnÃ©es sont stockÃ©es dans :
- **Docker** : `/app/chroma_db` (volume `chroma_data`)
- **Local** : `./chroma_db` Ã  la racine du projet

### Configuration PostgreSQL

La base de donnÃ©es est initialisÃ©e automatiquement avec Docker Compose. Pour une installation locale :

```sql
CREATE DATABASE ntic2;
CREATE USER ntic WITH PASSWORD 'ntic';
GRANT ALL PRIVILEGES ON DATABASE ntic2 TO ntic;
```

Puis exÃ©cuter `init.sql` pour crÃ©er les tables.

---

## ğŸ’» Utilisation

### DÃ©marrage Rapide

1. **DÃ©marrer tous les services**
```bash
docker-compose up -d
```

2. **VÃ©rifier le statut**
```bash
docker-compose ps
```

3. **Voir les logs**
```bash
docker-compose logs -f backend
```

4. **AccÃ©der Ã  l'application**
   - Frontend : http://localhost:8080
   - Backend API : http://localhost:5000

### Ingestion des DonnÃ©es

L'ingestion rÃ©cupÃ¨re le contenu du site web ISTA NTIC et le stocke dans ChromaDB.

```bash
# Avec Docker
docker-compose exec backend python -m app.rag.ingest

# Local
cd backend
python -m app.rag.ingest
```

**Options d'ingestion** :
- `--update-only` : Met Ã  jour uniquement les pages modifiÃ©es
- `--resume-from-backup` : Reprend depuis un backup

### Utilisation de l'API

#### Endpoint Chat

```bash
POST http://localhost:5000/api/chat
Content-Type: application/json

{
  "message": "Quels sont les emplois du temps disponibles ?",
  "user_id": "user123"
}
```

**RÃ©ponse** :
```json
{
  "reply": "Les emplois du temps sont disponibles...",
  "sources": [
    {
      "title": "ISTA NTIC SM - Emplois du temps",
      "section": "emplois-du-temps",
      "url": "https://sites.google.com/view/ista-ntic-sm/emplois-du-temps"
    }
  ],
  "status": {
    "chunks": 22,
    "connected": true
  },
  "rag_used": true,
  "chunk_count": 22,
  "sources_count": 1,
  "language": "fr"
}
```

#### Endpoint Status

```bash
GET http://localhost:5000/api/chat/status
```

#### Endpoint Clear

```bash
POST http://localhost:5000/api/chat/clear
Content-Type: application/json

{
  "user_id": "user123"
}
```

---

## ğŸ“š API Documentation

### POST /api/chat

Envoie un message Ã  l'assistant et reÃ§oit une rÃ©ponse.

**Request Body** :
```json
{
  "message": "string (requis)",
  "user_id": "string (optionnel, dÃ©faut: 'anon')"
}
```

**Response** :
```json
{
  "reply": "string - RÃ©ponse de l'assistant",
  "sources": [
    {
      "title": "string",
      "section": "string",
      "url": "string",
      "display": "string"
    }
  ],
  "status": {
    "chunks": "number",
    "connected": "boolean"
  },
  "rag_used": "boolean",
  "chunk_count": "number",
  "sources_count": "number",
  "language": "string (fr/en/ar/es)"
}
```

### GET /api/chat/status

Retourne le statut du systÃ¨me (nombre de chunks, connexion).

**Response** :
```json
{
  "chunks": 22,
  "connected": true,
  "status": "ok",
  "message": "22 chunks disponibles"
}
```

### POST /api/chat/clear

Efface l'historique de conversation pour un utilisateur.

**Request Body** :
```json
{
  "user_id": "string (requis)"
}
```

**Response** :
```json
{
  "status": "success",
  "message": "Conversation effacÃ©e"
}
```

---

## ğŸ” RAG Pipeline

### Architecture RAG

Le systÃ¨me RAG (Retrieval-Augmented Generation) fonctionne en plusieurs Ã©tapes :

1. **Ingestion** (`rag/ingest.py`)
   - Scraping du site web ISTA NTIC
   - Parsing HTML avec BeautifulSoup
   - Extraction du contenu textuel
   - DÃ©coupage en chunks
   - GÃ©nÃ©ration d'embeddings
   - Stockage dans ChromaDB

2. **Recherche** (`rag/pipeline.py`)
   - RequÃªte utilisateur
   - GÃ©nÃ©ration d'embedding de la requÃªte
   - Recherche vectorielle dans ChromaDB
   - RÃ©cupÃ©ration des chunks pertinents
   - Retour du contexte formatÃ©

3. **GÃ©nÃ©ration** (`agent/core.py`)
   - Injection du contexte RAG dans le prompt
   - Appel au LLM
   - GÃ©nÃ©ration de la rÃ©ponse
   - Formatage avec sources

### Embeddings

Le systÃ¨me supporte plusieurs mÃ©thodes d'embeddings :

1. **Ollama** (prioritÃ© 1) - Embeddings via modÃ¨le Ollama
2. **Sentence Transformers** (prioritÃ© 2) - ModÃ¨le local `paraphrase-multilingual-MiniLM-L12-v2`
3. **Hugging Face API** (prioritÃ© 3) - API gratuite
4. **OpenAI** (fallback) - `text-embedding-ada-002`

### Configuration RAG

Les paramÃ¨tres RAG sont configurables dans `rag/pipeline.py` :

```python
# Nombre de rÃ©sultats Ã  retourner
n_results = 7  # RecommandÃ©: 5-10

# Filtrage par section
filter_section = "emplois-du-temps"  # Optionnel
```

---

## ğŸ¤– Agent Core

### Architecture de l'Agent

L'agent (`agent/core.py`) est le cÅ“ur du systÃ¨me. Il orchestre :

1. **DÃ©tection de langue** - DÃ©tecte automatiquement la langue de la requÃªte
2. **Chargement de l'historique** - RÃ©cupÃ¨re les messages prÃ©cÃ©dents
3. **RÃ©cupÃ©ration RAG** - Obtient le contexte pertinent
4. **Appel LLM** - GÃ©nÃ¨re la rÃ©ponse avec fallback automatique
5. **Nettoyage** - Supprime les rÃ©pÃ©titions et formate la rÃ©ponse
6. **Sauvegarde** - Stocke l'Ã©change dans PostgreSQL

### Prompt SystÃ¨me

Le prompt systÃ¨me (`SYSTEM_PROMPT`) dÃ©finit le comportement de l'agent :

- âœ… RÃ©ponses uniquement basÃ©es sur les donnÃ©es RAG
- âœ… Format dÃ©taillÃ©, propre, simple et spÃ©cifique
- âœ… Support multilingue
- âœ… IntÃ©gration des sources et URLs
- âœ… Section SOURCES obligatoire

### Fallback Intelligent

Si le LLM n'est pas disponible, le systÃ¨me utilise un fallback intelligent qui :
- Extrait les informations pertinentes du contexte RAG
- Formate la rÃ©ponse de maniÃ¨re structurÃ©e
- Inclut les sources

---

## ğŸš¢ DÃ©ploiement

### DÃ©ploiement avec Docker Compose

1. **Production**
```bash
docker-compose -f docker-compose.yml up -d
```

2. **DÃ©veloppement**
```bash
docker-compose up frontend-dev backend postgres
```

### DÃ©ploiement sur Serveur

1. **Cloner le projet sur le serveur**
```bash
git clone <repository-url>
cd ntic2_ai_agent_production
```

2. **Configurer les variables d'environnement**
```bash
cp env.example .env
nano .env  # Ã‰diter avec vos configurations
```

3. **DÃ©marrer les services**
```bash
docker-compose up -d
```

4. **Configurer Nginx (optionnel)**
```nginx
server {
    listen 80;
    server_name votre-domaine.com;

    location / {
        proxy_pass http://localhost:8080;
    }

    location /api {
        proxy_pass http://localhost:5000;
    }
}
```

### Variables d'Environnement Production

```env
# SÃ©curitÃ©
FLASK_ENV=production
DEBUG=False

# LLM Provider
LLM_PROVIDER=groq  # RecommandÃ© pour production (rapide)

# Base de donnÃ©es
POSTGRES_PASSWORD=password_securise
```

---

## ğŸ”§ Troubleshooting

### ProblÃ¨mes Courants

#### 1. Erreur "Aucun provider LLM disponible"

**Solution** :
- VÃ©rifier que Ollama est dÃ©marrÃ© : `docker-compose ps ollama`
- VÃ©rifier les clÃ©s API dans `.env`
- VÃ©rifier les logs : `docker-compose logs backend`

#### 2. ChromaDB vide ou erreur de collection

**Solution** :
```bash
# RÃ©ingÃ©rer les donnÃ©es
docker-compose exec backend python -m app.rag.ingest
```

#### 3. PostgreSQL non accessible

**Solution** :
```bash
# VÃ©rifier que PostgreSQL est dÃ©marrÃ©
docker-compose ps postgres

# VÃ©rifier les logs
docker-compose logs postgres

# RecrÃ©er la base de donnÃ©es si nÃ©cessaire
docker-compose down -v
docker-compose up -d postgres
```

#### 4. Ollama timeout

**Solution** :
- Augmenter le timeout dans `agent/core.py`
- VÃ©rifier que le modÃ¨le est tÃ©lÃ©chargÃ© : `ollama list`
- TÃ©lÃ©charger le modÃ¨le : `ollama pull llama3.2:latest`

#### 5. Frontend ne se connecte pas au backend

**Solution** :
- VÃ©rifier que le backend est accessible : `curl http://localhost:5000`
- VÃ©rifier CORS dans `main.py`
- VÃ©rifier les URLs dans `frontend/src/components/Chat.jsx`

### Logs et Debugging

#### Voir les logs en temps rÃ©el
```bash
docker-compose logs -f backend
docker-compose logs -f frontend
```

#### Logs spÃ©cifiques
```bash
# Logs backend uniquement
docker-compose logs backend | grep ERROR

# Logs RAG
docker-compose logs backend | grep RAG
```

#### Diagnostic systÃ¨me
```bash
# Script de diagnostic
docker-compose exec backend python diagnostic_systeme.py
```

### Performance

#### Optimisations RecommandÃ©es

1. **ChromaDB** : Utiliser un nombre de rÃ©sultats adaptÃ© (5-7)
2. **LLM** : Utiliser Groq pour la vitesse
3. **Embeddings** : Utiliser Ollama pour la cohÃ©rence
4. **PostgreSQL** : Indexer les colonnes frÃ©quemment utilisÃ©es

#### Monitoring

- **Chunks disponibles** : `GET /api/chat/status`
- **Logs** : `docker-compose logs`
- **Ressources** : `docker stats`

---

## ğŸ“Š MÃ©triques et ObservabilitÃ©

### Logs

Le systÃ¨me gÃ©nÃ¨re des logs dÃ©taillÃ©s :
- **INFO** : OpÃ©rations normales
- **WARNING** : ProblÃ¨mes non critiques
- **ERROR** : Erreurs nÃ©cessitant attention

### MÃ©triques Disponibles

- Nombre de chunks dans ChromaDB
- Nombre de sources par rÃ©ponse
- Langue dÃ©tectÃ©e
- Provider LLM utilisÃ©
- Temps de rÃ©ponse

---

## ğŸ” SÃ©curitÃ©

### Bonnes Pratiques

1. **Ne jamais commiter les secrets**
   - `.env` doit Ãªtre dans `.gitignore`
   - `secrets/` doit Ãªtre dans `.gitignore`

2. **Utiliser des mots de passe forts**
   - PostgreSQL : changer le mot de passe par dÃ©faut
   - API Keys : rÃ©gÃ©nÃ©rer rÃ©guliÃ¨rement

3. **Limiter l'accÃ¨s**
   - Utiliser un firewall
   - Limiter les ports exposÃ©s
   - Utiliser HTTPS en production

4. **Mettre Ã  jour rÃ©guliÃ¨rement**
   - Docker images
   - DÃ©pendances Python
   - DÃ©pendances Node.js

---

## ğŸ“ Contribution

### Structure de Code

- **Backend** : PEP 8 (Python style guide)
- **Frontend** : ESLint + Prettier
- **Commits** : Messages clairs et descriptifs

### Tests

```bash
# Tests backend
cd backend
python test_functionnalites.py

# Tests unitaires
python test_unitaire.py
```

---

## ğŸ“„ Licence

Ce projet est propriÃ©taire et destinÃ© Ã  l'usage interne de l'ISTA NTIC.

---

## ğŸ‘¥ Support

Pour toute question ou problÃ¨me :
- **Email** : istanticsm@gmail.com
- **Documentation** : Ce fichier README.md
- **Logs** : VÃ©rifier les logs Docker

---

## ğŸ¯ Roadmap

### AmÃ©liorations Futures

- [ ] Support de plus de langues
- [ ] AmÃ©lioration de la dÃ©tection de section
- [ ] Cache des rÃ©ponses frÃ©quentes
- [ ] Interface d'administration
- [ ] Analytics et mÃ©triques avancÃ©es
- [ ] Support de fichiers PDF/Word dans RAG
- [ ] Mode hors ligne complet
- [ ] IntÃ©gration avec d'autres systÃ¨mes

---

**DerniÃ¨re mise Ã  jour** : 2024
**Version** : 1.0.0
**Auteur** : Ã‰quipe ISTA NTIC
